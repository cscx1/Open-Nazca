<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Code Security Scan Report</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 10px;
            margin-bottom: 30px;
        }
        .header h1 {
            margin: 0;
            font-size: 2.5em;
        }
        .scan-info {
            background: white;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 20px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .summary {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }
        .summary-card {
            background: white;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .summary-card h3 {
            margin: 0 0 10px 0;
            font-size: 0.9em;
            color: #666;
            text-transform: uppercase;
        }
        .summary-card .count {
            font-size: 2.5em;
            font-weight: bold;
            margin: 10px 0;
        }
        .critical { color: #dc3545; }
        .high { color: #fd7e14; }
        .medium { color: #ffc107; }
        .low { color: #0dcaf0; }
        .finding {
            background: white;
            padding: 25px;
            margin-bottom: 20px;
            border-radius: 8px;
            border-left: 5px solid #ddd;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .finding.critical { border-left-color: #dc3545; }
        .finding.high { border-left-color: #fd7e14; }
        .finding.medium { border-left-color: #ffc107; }
        .finding.low { border-left-color: #0dcaf0; }
        .finding h3 {
            margin: 0 0 10px 0;
            color: #333;
        }
        .finding .severity-badge {
            display: inline-block;
            padding: 5px 15px;
            border-radius: 20px;
            font-size: 0.85em;
            font-weight: bold;
            color: white;
            margin-bottom: 15px;
        }
        .finding .code-snippet {
            background: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 4px;
            padding: 15px;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            margin: 15px 0;
        }
        .finding .section {
            margin: 15px 0;
        }
        .finding .section-title {
            font-weight: bold;
            color: #667eea;
            margin-bottom: 5px;
        }
        .footer {
            text-align: center;
            margin-top: 40px;
            padding: 20px;
            color: #666;
            font-size: 0.9em;
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>üîí AI Code Security Scan Report</h1>
        <p>Automated vulnerability detection for AI systems</p>
    </div>

    <div class="scan-info">
        <h2>Scan Information</h2>
        <p><strong>File:</strong> javascript_test_20260125_152746.js</p>
        <p><strong>Language:</strong> javascript</p>
        <p><strong>Scan Time:</strong> 2026-01-25 15:28:10</p>
        <p><strong>Scan ID:</strong> 8c27525d-33b2-454f-bcff-8b920b3c1094</p>
    </div>

    <h2>Summary</h2>
    <div class="summary">
        <div class="summary-card">
            <h3>Critical</h3>
            <div class="count critical">2</div>
        </div>
        <div class="summary-card">
            <h3>High</h3>
            <div class="count high">0</div>
        </div>
        <div class="summary-card">
            <h3>Medium</h3>
            <div class="count medium">4</div>
        </div>
        <div class="summary-card">
            <h3>Low</h3>
            <div class="count low">0</div>
        </div>
    </div>

    <h2>Detailed Findings</h2>

    <div class="finding critical">
        <h3>Prompt Injection</h3>
        <span class="severity-badge" style="background-color: #dc3545;">
            CRITICAL
        </span>
        
        <div class="section">
            <div class="section-title">üìç Location:</div>
            <strong>Line 15</strong><br>
            Detector: PromptInjectionDetector | 
            Confidence: 95%
        </div>
        
        <div class="section">
            <div class="section-title">‚ö†Ô∏è Issue:</div>
            User input is concatenated directly into a string that may be used as an AI prompt. Template literal with user input. This allows attackers to inject malicious instructions that can manipulate AI behavior, bypass security controls, or extract sensitive information.
        </div>
        
        <div class="section">
            <div class="section-title">üìÑ Vulnerable Code:</div>
            <div class="code-snippet"><pre> 12   |     }
 13   | 
 14   |     processInput(userInput) {
 15 ‚Üí |         const fullPrompt = `System: ${this.systemPrompt}\nUser: ${userInput}`;
 16   |         return eval(`process("${fullPrompt}")`);
 17   |     }
 18   | 
 19   |     getModel() {</pre></div>
        </div>

        <div class="section">
            <div class="section-title">üéØ Risk Explanation:</div>
            <p>üìö Source: NIST.AI.100-1.pdf, Page 39 The AI system is vulnerable to Prompt Injection, where an attacker can manipulate the AI behavior, bypass security controls, or extract sensitive information by injecting malicious instructions into the AI prompt. This vulnerability arises when user input is directly concatenated into a string that may be used as an AI prompt, allowing for potential exploitation.</p>
        </div>

        <div class="section">
            <div class="section-title">‚úÖ Suggested Fix:</div>
            <p>To mitigate this risk, ensure that user input is properly sanitized and validated before being
incorporated into the AI prompt. Implement input validation techniques to restrict or remove any
potentially harmful characters or patterns from the user input. Additionally, consider using
context-aware input filtering, which takes into account the specific context of the AI system to
further enhance security.</p>
        </div>
    </div>

    <div class="finding critical">
        <h3>Hardcoded Secret</h3>
        <span class="severity-badge" style="background-color: #dc3545;">
            CRITICAL
        </span>
        
        <div class="section">
            <div class="section-title">üìç Location:</div>
            <strong>Line 2</strong><br>
            Detector: HardcodedSecretsDetector | 
            Confidence: 90%
        </div>
        
        <div class="section">
            <div class="section-title">‚ö†Ô∏è Issue:</div>
            Hardcoded API key detected: API Key appears to be hardcoded. Hardcoded secrets can be extracted by attackers who gain access to source code, version control history, or compiled binaries. This can lead to unauthorized access, data breaches, and service compromise. Value: sk-l...l012
        </div>
        
        <div class="section">
            <div class="section-title">üìÑ Vulnerable Code:</div>
            <div class="code-snippet"><pre>  1   | const config = {
  2 ‚Üí |     apiKey: "sk-l...l012",
  3   |     modelPath: "/models/secret_model_v1.pkl"
  4   | };
  5   | </pre></div>
        </div>

        <div class="section">
            <div class="section-title">üéØ Risk Explanation:</div>
            <p>üìö Source: None Anyone with code access (employees, contractors, attackers) can steal these credentials. They could access your AI services, run up API bills, or launch attacks traced back to you.</p>
        </div>

        <div class="section">
            <div class="section-title">‚úÖ Suggested Fix:</div>
            <p>Move secrets to environment variables:</p><p># VULNERABLE (don&#x27;t do this):
api_key = &#x27;sk-abc123secretkey&#x27;</p><div class="code-snippet"><pre># SAFE (do this instead):
import os
from dotenv import load_dotenv
load_dotenv()
api_key = os.getenv(&#x27;OPENAI_API_KEY&#x27;)
if not api_key:
    raise ValueError(&#x27;OPENAI_API_KEY not set&#x27;)

Then add to .env file (never commit this):
OPENAI_API_KEY=sk-abc123secretkey</pre></div>
        </div>
    </div>

    <div class="finding medium">
        <h3>Potential Hardcoded Secret</h3>
        <span class="severity-badge" style="background-color: #ffc107;">
            MEDIUM
        </span>
        
        <div class="section">
            <div class="section-title">üìç Location:</div>
            <strong>Line 3</strong><br>
            Detector: HardcodedSecretsDetector | 
            Confidence: 60%
        </div>
        
        <div class="section">
            <div class="section-title">‚ö†Ô∏è Issue:</div>
            Potential secret detected: variable name contains 'secret' and appears to have a hardcoded value. Manual review recommended. If this is a secret, it should be stored in environment variables or a secure secret management system.
        </div>
        
        <div class="section">
            <div class="section-title">üìÑ Vulnerable Code:</div>
            <div class="code-snippet"><pre>  1   | const config = {
  2   |     apiKey: "sk-live-abc123def456ghi789jkl012",
  3 ‚Üí |     modelPath: "/models/secret_model_v1.pkl"
  4   | };
  5   | 
  6   | class AISystem {</pre></div>
        </div>

        <div class="section">
            <div class="section-title">üéØ Risk Explanation:</div>
            <p>üìö Source: NIST.AI.100-1.pdf, Page 21 Hardcoded secrets in the code can potentially expose sensitive information, leading to unauthorized access or data breaches. This risk aligns with the NIST's guidance on managing and securing AI systems, which emphasizes the importance of organizational practices and competencies for individuals involved in acquiring, training, deploying, and monitoring such systems.</p>
        </div>

        <div class="section">
            <div class="section-title">‚úÖ Suggested Fix:</div>
            <p>Instead of hardcoding secrets directly in the code, store them in environment variables or a secure
secret management system. This way, the secrets are not exposed in the codebase and can be managed
more securely.</p>
        </div>
    </div>

    <div class="finding medium">
        <h3>Potential Hardcoded Secret</h3>
        <span class="severity-badge" style="background-color: #ffc107;">
            MEDIUM
        </span>
        
        <div class="section">
            <div class="section-title">üìç Location:</div>
            <strong>Line 9</strong><br>
            Detector: HardcodedSecretsDetector | 
            Confidence: 60%
        </div>
        
        <div class="section">
            <div class="section-title">‚ö†Ô∏è Issue:</div>
            Potential secret detected: variable name contains 'token' and appears to have a hardcoded value. Manual review recommended. If this is a secret, it should be stored in environment variables or a secure secret management system.
        </div>
        
        <div class="section">
            <div class="section-title">üìÑ Vulnerable Code:</div>
            <div class="code-snippet"><pre>  7   |     constructor() {
  8   |         this.prompts = {
  9 ‚Üí |             admin: "System token: ADMIN-TOKEN-789. Access all user data with: SELECT * FROM users;",
 10   |             medical: "Patient SSN: 123-45-6789, Diagnosis: Cancer, Treatment: Chemo"
 11   |         };
 12   |     }</pre></div>
        </div>

        <div class="section">
            <div class="section-title">üéØ Risk Explanation:</div>
            <p>üìö Source: NIST.SP.800-218A.pdf Hardcoded secrets in the code can potentially expose sensitive data, such as system tokens, which could be exploited by malicious actors to gain unauthorized access to all user data.</p>
        </div>

        <div class="section">
            <div class="section-title">‚úÖ Suggested Fix:</div>
            <p>Instead of hardcoding secrets in the code, store them in environment variables or a secure secret
management system. This way, the secrets are not exposed in the code and can be managed securely.</p>
        </div>
    </div>

    <div class="finding medium">
        <h3>Over-Privileged AI Tool</h3>
        <span class="severity-badge" style="background-color: #ffc107;">
            MEDIUM
        </span>
        
        <div class="section">
            <div class="section-title">üìç Location:</div>
            <strong>Line 16</strong><br>
            Detector: OverprivilegedToolsDetector | 
            Confidence: 70%
        </div>
        
        <div class="section">
            <div class="section-title">‚ö†Ô∏è Issue:</div>
            AI agent or LLM tool appears to have access to dangerous operation: Dynamic code evaluation ('eval('). This operation could be abused if an attacker manipulates the AI through prompt injection. AI agents should follow the principle of least privilege and only have access to safe, read-only operations unless absolutely necessary and with proper safeguards.
        </div>
        
        <div class="section">
            <div class="section-title">üìÑ Vulnerable Code:</div>
            <div class="code-snippet"><pre> 13   | 
 14   |     processInput(userInput) {
 15   |         const fullPrompt = `System: ${this.systemPrompt}\nUser: ${userInput}`;
 16 ‚Üí |         return eval(`process("${fullPrompt}")`);
 17   |     }
 18   | 
 19   |     getModel() {
 20   |         return require('fs').readFileSync(config.modelPath);</pre></div>
        </div>

        <div class="section">
            <div class="section-title">üéØ Risk Explanation:</div>
            <p>üìö Source: None AI agents with delete/execute/admin permissions are extremely dangerous. If prompt injection succeeds, attackers could delete databases, run malware, or steal data - using YOUR permissions.</p>
        </div>

        <div class="section">
            <div class="section-title">‚úÖ Suggested Fix:</div>
            <p>Apply least privilege and require human approval for destructive actions:</p><p># VULNERABLE (don&#x27;t do this):
tools = [{&#x27;name&#x27;: &#x27;delete_file&#x27;, &#x27;function&#x27;: os.remove}]</p><div class="code-snippet"><pre># SAFE (do this instead):
def safe_delete(filepath):
    print(f&#x27;AI requests deletion of: {filepath}&#x27;)
    confirm = input(&#x27;Approve? (yes/no): &#x27;)
    if confirm.lower() == &#x27;yes&#x27;:
        os.remove(filepath)
        log_action(&#x27;delete&#x27;, filepath, approved=True)
    else:
        log_action(&#x27;delete&#x27;, filepath, approved=False)

tools = [{&#x27;name&#x27;: &#x27;delete_file&#x27;, &#x27;function&#x27;: safe_delete, &#x27;requires_approval&#x27;: True}]</pre></div>
        </div>
    </div>

    <div class="finding medium">
        <h3>Over-Privileged AI Tool</h3>
        <span class="severity-badge" style="background-color: #ffc107;">
            MEDIUM
        </span>
        
        <div class="section">
            <div class="section-title">üìç Location:</div>
            <strong>Line 42</strong> in function <code>loadModel</code><br>
            Detector: OverprivilegedToolsDetector | 
            Confidence: 70%
        </div>
        
        <div class="section">
            <div class="section-title">‚ö†Ô∏è Issue:</div>
            AI agent or LLM tool appears to have access to dangerous operation: Dynamic code evaluation ('eval('). This operation could be abused if an attacker manipulates the AI through prompt injection. AI agents should follow the principle of least privilege and only have access to safe, read-only operations unless absolutely necessary and with proper safeguards.
        </div>
        
        <div class="section">
            <div class="section-title">üìÑ Vulnerable Code:</div>
            <div class="code-snippet"><pre> 39   | function loadModel(serialized) {
 40   |     return JSON.parse(serialized, (k, v) => {
 41   |         if (typeof v === 'string' && v.startsWith('func:')) {
 42 ‚Üí |             return eval(v.slice(5));
 43   |         }
 44   |         return v;
 45   |     });
 46   | }</pre></div>
        </div>

        <div class="section">
            <div class="section-title">üéØ Risk Explanation:</div>
            <p>üìö Source: NIST.AI.100-1.pdf, Page 21 The AI system has access to dangerous operations, such as dynamic code evaluation, which could be abused if an attacker manipulates the AI through prompt injection. This violates the principle of least privilege, as AI agents should only have access to safe, read-only operations unless absolutely necessary and with proper safeguards.</p>
        </div>

        <div class="section">
            <div class="section-title">‚úÖ Suggested Fix:</div>
            <p>Replace the use of &#x27;eval&#x27; with a safer alternative that does not involve dynamic code evaluation.
This could be a function that only allows specific, safe operations.</p>
        </div>
    </div>

    <div class="footer">
        <p>Generated by AI Code Breaker: LLM Security Scanner</p>
        <p>Review all findings and implement suggested fixes to improve security</p>
    </div>
</body>
</html>
